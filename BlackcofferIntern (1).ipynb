{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "556c89dd-7926-4a78-a671-7e156f31af61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: C:\\Users\\sunik\n"
     ]
    }
   ],
   "source": [
    "# Imports & Configuration\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import os\n",
    "\n",
    "# Download necessary NLTK resources\n",
    "for resource in [\"punkt\", \"stopwords\"]:\n",
    "    try:\n",
    "        nltk.data.find(f\"tokenizers/{resource}\" if resource == \"punkt\" else f\"corpora/{resource}\")\n",
    "    except LookupError:\n",
    "        nltk.download(resource)\n",
    "\n",
    "# Confirm working directory\n",
    "print(\"Current working directory:\", os.getcwd())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "34b8fb66-e2e0-4dcb-a30b-a94af27a3f6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 147 URLs from Input.xlsx\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URL_ID</th>\n",
       "      <th>URL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bctech2011</td>\n",
       "      <td>https://insights.blackcoffer.com/ml-and-ai-bas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bctech2012</td>\n",
       "      <td>https://insights.blackcoffer.com/streamlined-i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bctech2013</td>\n",
       "      <td>https://insights.blackcoffer.com/efficient-dat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>bctech2014</td>\n",
       "      <td>https://insights.blackcoffer.com/effective-man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bctech2015</td>\n",
       "      <td>https://insights.blackcoffer.com/streamlined-t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       URL_ID                                                URL\n",
       "0  bctech2011  https://insights.blackcoffer.com/ml-and-ai-bas...\n",
       "1  bctech2012  https://insights.blackcoffer.com/streamlined-i...\n",
       "2  bctech2013  https://insights.blackcoffer.com/efficient-dat...\n",
       "3  bctech2014  https://insights.blackcoffer.com/effective-man...\n",
       "4  bctech2015  https://insights.blackcoffer.com/streamlined-t..."
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load Input file directly from your Jupyter home directory\n",
    "data = pd.read_excel(\"Input.xlsx\")\n",
    "\n",
    "print(f\"Loaded {len(data)} URLs from Input.xlsx\")\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f17e134e-eb53-4ab7-a5aa-2bfcc872ddc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 1/147: bctech2011\n",
      "Processing 2/147: bctech2012\n",
      "Processing 3/147: bctech2013\n",
      "Processing 4/147: bctech2014\n",
      "Processing 5/147: bctech2015\n",
      "Processing 6/147: bctech2016\n",
      "Processing 7/147: bctech2017\n",
      "Processing 8/147: bctech2018\n",
      "Processing 9/147: bctech2019\n",
      "Processing 10/147: bctech2020\n",
      "Processing 11/147: bctech2021\n",
      "Processing 12/147: bctech2022\n",
      "Processing 13/147: bctech2023\n",
      "Processing 14/147: bctech2024\n",
      "Processing 15/147: bctech2025\n",
      "Processing 16/147: bctech2026\n",
      "Processing 17/147: bctech2027\n",
      "Processing 18/147: bctech2028\n",
      "Processing 19/147: bctech2029\n",
      "Processing 20/147: bctech2030\n",
      "Processing 21/147: bctech2031\n",
      "Processing 22/147: bctech2032\n",
      "Processing 23/147: bctech2033\n",
      "Processing 24/147: bctech2034\n",
      "Processing 25/147: bctech2035\n",
      "Processing 26/147: bctech2036\n",
      "Processing 27/147: bctech2037\n",
      "Processing 28/147: bctech2038\n",
      "Processing 29/147: bctech2039\n",
      "Processing 30/147: bctech2040\n",
      "Processing 31/147: bctech2041\n",
      "Processing 32/147: bctech2042\n",
      "Processing 33/147: bctech2043\n",
      "Processing 34/147: bctech2044\n",
      "Processing 35/147: bctech2045\n",
      "Processing 36/147: bctech2046\n",
      "Processing 37/147: bctech2047\n",
      "Processing 38/147: bctech2048\n",
      "Processing 39/147: bctech2049\n",
      "Processing 40/147: bctech2050\n",
      "Processing 41/147: bctech2051\n",
      "Processing 42/147: bctech2052\n",
      "Processing 43/147: bctech2053\n",
      "Processing 44/147: bctech2054\n",
      "Processing 45/147: bctech2055\n",
      "Processing 46/147: bctech2056\n",
      "Processing 47/147: bctech2057\n",
      "Processing 48/147: bctech2058\n",
      "Processing 49/147: bctech2059\n",
      "Processing 50/147: bctech2060\n",
      "Processing 51/147: bctech2061\n",
      "Processing 52/147: bctech2062\n",
      "Processing 53/147: bctech2063\n",
      "Processing 54/147: bctech2064\n",
      "Processing 55/147: bctech2065\n",
      "Processing 56/147: bctech2066\n",
      "Processing 57/147: bctech2067\n",
      "Processing 58/147: bctech2068\n",
      "Processing 59/147: bctech2069\n",
      "Processing 60/147: bctech2070\n",
      "Processing 61/147: bctech2071\n",
      "Processing 62/147: bctech2072\n",
      "Processing 63/147: bctech2073\n",
      "Processing 64/147: bctech2074\n",
      "Processing 65/147: bctech2075\n",
      "Processing 66/147: bctech2076\n",
      "Processing 67/147: bctech2077\n",
      "Processing 68/147: bctech2078\n",
      "Processing 69/147: bctech2079\n",
      "Processing 70/147: bctech2080\n",
      "Processing 71/147: bctech2081\n",
      "Processing 72/147: bctech2082\n",
      "Processing 73/147: bctech2083\n",
      "Processing 74/147: bctech2084\n",
      "Processing 75/147: bctech2085\n",
      "Processing 76/147: bctech2086\n",
      "Processing 77/147: bctech2087\n",
      "Processing 78/147: bctech2088\n",
      "Processing 79/147: bctech2089\n",
      "Processing 80/147: bctech2090\n",
      "Processing 81/147: bctech2091\n",
      "Processing 82/147: bctech2092\n",
      "Processing 83/147: bctech2093\n",
      "Processing 84/147: bctech2094\n",
      "Processing 85/147: bctech2095\n",
      "Processing 86/147: bctech2096\n",
      "Processing 87/147: bctech2097\n",
      "Processing 88/147: bctech2098\n",
      "Processing 89/147: bctech2099\n",
      "Processing 90/147: bctech2100\n",
      "Processing 91/147: bctech2101\n",
      "Processing 92/147: bctech2102\n",
      "Processing 93/147: bctech2103\n",
      "Processing 94/147: bctech2104\n",
      "Processing 95/147: bctech2105\n",
      "Processing 96/147: bctech2106\n",
      "Processing 97/147: bctech2107\n",
      "Processing 98/147: bctech2108\n",
      "Processing 99/147: bctech2109\n",
      "Processing 100/147: bctech2110\n",
      "Processing 101/147: bctech2111\n",
      "Processing 102/147: bctech2112\n",
      "Processing 103/147: bctech2113\n",
      "Processing 104/147: bctech2114\n",
      "Processing 105/147: bctech2115\n",
      "Processing 106/147: bctech2116\n",
      "Processing 107/147: bctech2117\n",
      "Processing 108/147: bctech2118\n",
      "Processing 109/147: bctech2119\n",
      "Processing 110/147: bctech2120\n",
      "Processing 111/147: bctech2121\n",
      "Processing 112/147: bctech2122\n",
      "Processing 113/147: bctech2123\n",
      "Processing 114/147: bctech2124\n",
      "Processing 115/147: bctech2125\n",
      "Processing 116/147: bctech2126\n",
      "Processing 117/147: bctech2127\n",
      "Processing 118/147: bctech2128\n",
      "Processing 119/147: bctech2129\n",
      "Processing 120/147: bctech2130\n",
      "Processing 121/147: bctech2131\n",
      "Processing 122/147: bctech2132\n",
      "Processing 123/147: bctech2133\n",
      "Processing 124/147: bctech2134\n",
      "Processing 125/147: bctech2135\n",
      "Processing 126/147: bctech2136\n",
      "Processing 127/147: bctech2137\n",
      "Processing 128/147: bctech2138\n",
      "Processing 129/147: bctech2139\n",
      "Processing 130/147: bctech2140\n",
      "Processing 131/147: bctech2141\n",
      "Processing 132/147: bctech2142\n",
      "Processing 133/147: bctech2143\n",
      "Processing 134/147: bctech2144\n",
      "Processing 135/147: bctech2145\n",
      "Processing 136/147: bctech2146\n",
      "Processing 137/147: bctech2147\n",
      "Processing 138/147: bctech2148\n",
      "Processing 139/147: bctech2149\n",
      "Processing 140/147: bctech2150\n",
      "Processing 141/147: bctech2151\n",
      "Processing 142/147: bctech2152\n",
      "Processing 143/147: bctech2153\n",
      "Processing 144/147: bctech2154\n",
      "Processing 145/147: bctech2155\n",
      "Processing 146/147: bctech2156\n",
      "Processing 147/147: bctech2157\n",
      "Scraping complete — 0 URLs failed.\n"
     ]
    }
   ],
   "source": [
    "# Create empty lists for scraped results\n",
    "titles = []\n",
    "texts = []\n",
    "failed_urls = []\n",
    "\n",
    "for index, row in data.iterrows():\n",
    "    url_id = row[\"URL_ID\"]\n",
    "    url = row[\"URL\"]\n",
    "    print(f\"Processing {index+1}/{len(data)}: {url_id}\")\n",
    "\n",
    "    try:\n",
    "        # Send request\n",
    "        response = requests.get(url, timeout=30)\n",
    "        response.raise_for_status()\n",
    "        bsoup = bs(response.text, \"html.parser\")\n",
    "\n",
    "        # Extract title\n",
    "        title_tag = bsoup.find(\"h1\", class_=\"entry-title\") or bsoup.find(\"title\")\n",
    "        article_title = title_tag.get_text(strip=True) if title_tag else \"TITLE NOT FOUND\"\n",
    "\n",
    "        # Extract article text\n",
    "        content_tag = bsoup.find(\"div\", class_=\"td-post-content tagdiv-type\") or bsoup.find(\"div\", class_=\"tdb-block-inner td-fix-index\")\n",
    "        article_text = content_tag.get_text(separator=\" \", strip=True) if content_tag else \"ARTICLE TEXT NOT FOUND\"\n",
    "\n",
    "        titles.append(article_title)\n",
    "        texts.append(article_text)\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\" Failed to fetch URL_ID {url_id}: {e}\")\n",
    "        titles.append(\"ERROR\")\n",
    "        texts.append(\"ERROR\")\n",
    "        failed_urls.append(url_id)\n",
    "\n",
    "data[\"Article_Title\"] = titles\n",
    "data[\"Article_Text\"] = texts\n",
    "print(f\"Scraping complete — {len(failed_urls)} URLs failed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9f96cd3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 12797 stopwords, 2006 positive, 4783 negative words.\n"
     ]
    }
   ],
   "source": [
    "# Load stopword files\n",
    "def load_wordlist(path):\n",
    "    words = set()\n",
    "    with open(path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "        for line in f:\n",
    "            w = line.strip()\n",
    "            if w and not w.startswith(\";\"):\n",
    "                words.add(w.lower())\n",
    "    return words\n",
    "\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "# Adding custom stopword lists (merge all uploaded ones)\n",
    "custom_stop_files = [\n",
    "    \"StopWords_Auditor.txt\", \"StopWords_Currencies.txt\", \"StopWords_DatesandNumbers.txt\",\n",
    "    \"StopWords_Generic.txt\", \"StopWords_GenericLong.txt\", \"StopWords_Geographic.txt\",\n",
    "    \"StopWords_Names.txt\"\n",
    "]\n",
    "for file in custom_stop_files:\n",
    "    stop_words.update(load_wordlist(file))\n",
    "\n",
    "# Load positive and negative words\n",
    "positive_words = load_wordlist(\"positive-words.txt\")\n",
    "negative_words = load_wordlist(\"negative-words.txt\")\n",
    "\n",
    "print(f\"Loaded {len(stop_words)} stopwords, {len(positive_words)} positive, {len(negative_words)} negative words.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4578253b-a080-49af-bca1-d120925ea528",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://insights.blackcoffer.com/amazon-buy-bot-an-automation-ai-tool-to-auto-checkouts/\n"
     ]
    }
   ],
   "source": [
    "test_url = 'https://insights.blackcoffer.com/efficient-aws-infrastructure-setup-and-management-addressing-security-scalability-and-compliance/'\n",
    "print(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "47719cdb-6817-4aa6-88c1-396a1c176998",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 147/147 [04:33<00:00,  1.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Scraping complete. 0 URLs failed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Full scraping loop for all articles\n",
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from tqdm import tqdm\n",
    "\n",
    "extracted_titles = []\n",
    "extracted_texts = []\n",
    "failed_urls = []\n",
    "\n",
    "for index, row in tqdm(data.iterrows(), total=len(data)):\n",
    "    url_id = row['URL_ID']\n",
    "    url = row['URL']\n",
    "\n",
    "    try:\n",
    "        response = requests.get(url, timeout=60)\n",
    "        response.raise_for_status()\n",
    "        bsoup = bs(response.text, 'html.parser')\n",
    "\n",
    "        title_tag = bsoup.find('h1', class_='entry-title') or bsoup.find('title')\n",
    "        article_title = title_tag.get_text().strip() if title_tag else \"TITLE NOT FOUND\"\n",
    "\n",
    "        content_tag = bsoup.find('div', class_='td-post-content tagdiv-type') or \\\n",
    "                      bsoup.find('div', class_='tdb-block-inner td-fix-index')\n",
    "        article_text = content_tag.get_text(separator=\"\\n\").strip() if content_tag else \"ARTICLE TEXT NOT FOUND\"\n",
    "\n",
    "        extracted_titles.append(article_title)\n",
    "        extracted_texts.append(article_text)\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\" Failed URL_ID {url_id} ({url}): {e}\")\n",
    "        extracted_titles.append(\"ERROR\")\n",
    "        extracted_texts.append(\"ERROR\")\n",
    "        failed_urls.append(url_id)\n",
    "\n",
    "data[\"Article_Title\"] = extracted_titles\n",
    "data[\"Article_Text\"] = extracted_texts\n",
    "\n",
    "print(f\" Scraping complete. {len(failed_urls)} URLs failed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e619b466",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       URL_ID                                                URL  \\\n",
      "0  bctech2011  https://insights.blackcoffer.com/ml-and-ai-bas...   \n",
      "1  bctech2012  https://insights.blackcoffer.com/streamlined-i...   \n",
      "2  bctech2013  https://insights.blackcoffer.com/efficient-dat...   \n",
      "3  bctech2014  https://insights.blackcoffer.com/effective-man...   \n",
      "4  bctech2015  https://insights.blackcoffer.com/streamlined-t...   \n",
      "\n",
      "                                       Article_Title  \\\n",
      "0  ML and AI-based insurance premium model to pre...   \n",
      "1  Streamlined Integration: Interactive Brokers A...   \n",
      "2  Efficient Data Integration and User-Friendly I...   \n",
      "3  Effective Management of Social Media Data Extr...   \n",
      "4  Streamlined Trading Operations Interface for M...   \n",
      "\n",
      "                                        Article_Text  \n",
      "0  Client Background\\n\\n\\nClient:\\n A leading ins...  \n",
      "1  Client Background\\n\\n\\nClient:\\n A leading fin...  \n",
      "2  Client Background\\n\\n\\nClient:\\n A leading tec...  \n",
      "3  Client Background\\n\\n\\nClient:\\n A leading tec...  \n",
      "4  Client Background\\n\\n\\nClient:\\n A leading fin...  \n"
     ]
    }
   ],
   "source": [
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "02b8bc59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "def count_syllables(word):\n",
    "    \"\"\"Estimate syllables in a word.\"\"\"\n",
    "    word = word.lower()\n",
    "    vowels = \"aeiou\"\n",
    "    count, prev_vowel = 0, False\n",
    "    for ch in word:\n",
    "        if ch in vowels:\n",
    "            if not prev_vowel:\n",
    "                count += 1\n",
    "            prev_vowel = True\n",
    "        else:\n",
    "            prev_vowel = False\n",
    "    if word.endswith((\"es\", \"ed\")):\n",
    "        count -= 1\n",
    "    return max(1, count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "04c88718",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_text(text):\n",
    "    \"\"\"Compute sentiment and readability metrics for one article.\"\"\"\n",
    "    if not isinstance(text, str) or text.strip() in [\"ERROR\", \"ARTICLE TEXT NOT FOUND\"]:\n",
    "        return {col: 0 for col in [\n",
    "            \"POSITIVE SCORE\", \"NEGATIVE SCORE\", \"POLARITY SCORE\", \"SUBJECTIVITY SCORE\",\n",
    "            \"AVG SENTENCE LENGTH\", \"PERCENTAGE OF COMPLEX WORDS\", \"FOG INDEX\",\n",
    "            \"AVG NUMBER OF WORDS PER SENTENCE\", \"COMPLEX WORD COUNT\",\n",
    "            \"WORD COUNT\", \"SYLLABLE PER WORD\", \"PERSONAL PRONOUNS\", \"AVG WORD LENGTH\"\n",
    "        ]}\n",
    "\n",
    "    sentences = sent_tokenize(text)\n",
    "    words = [w for w in word_tokenize(text) if w.isalpha()]\n",
    "    words_clean = [w.upper() for w in words if w.lower() not in stop_words]\n",
    "\n",
    "    # Sentiment\n",
    "    pos = sum(1 for w in words_clean if w in positive_words)\n",
    "    neg = sum(1 for w in words_clean if w in negative_words)\n",
    "    polarity = (pos - neg) / ((pos + neg) + 1e-6)\n",
    "    subjectivity = (pos + neg) / (len(words_clean) + 1e-6)\n",
    "\n",
    "    # Readability\n",
    "    avg_sentence_len = len(words_clean) / max(1, len(sentences))\n",
    "    complex_words = [w for w in words_clean if count_syllables(w) > 2]\n",
    "    percent_complex = len(complex_words) / max(1, len(words_clean))\n",
    "    fog_index = 0.4 * (avg_sentence_len + percent_complex)\n",
    "\n",
    "    # Other metrics\n",
    "    word_count = len(words_clean)\n",
    "    syllables = sum(count_syllables(w) for w in words_clean)\n",
    "    syllable_per_word = syllables / max(1, len(words_clean))\n",
    "    pronouns = len(re.findall(r\"\\b(I|we|my|ours|us)\\b\", text, flags=re.I))\n",
    "    avg_word_len = sum(len(w) for w in words_clean) / max(1, len(words_clean))\n",
    "\n",
    "    return {\n",
    "        \"POSITIVE SCORE\": pos,\n",
    "        \"NEGATIVE SCORE\": neg,\n",
    "        \"POLARITY SCORE\": polarity,\n",
    "        \"SUBJECTIVITY SCORE\": subjectivity,\n",
    "        \"AVG SENTENCE LENGTH\": avg_sentence_len,\n",
    "        \"PERCENTAGE OF COMPLEX WORDS\": percent_complex,\n",
    "        \"FOG INDEX\": fog_index,\n",
    "        \"AVG NUMBER OF WORDS PER SENTENCE\": avg_sentence_len,\n",
    "        \"COMPLEX WORD COUNT\": len(complex_words),\n",
    "        \"WORD COUNT\": word_count,\n",
    "        \"SYLLABLE PER WORD\": syllable_per_word,\n",
    "        \"PERSONAL PRONOUNS\": pronouns,\n",
    "        \"AVG WORD LENGTH\": avg_word_len\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4d83f506",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Running NLP analysis on all scraped articles...\n",
      " NLP analysis complete.\n"
     ]
    }
   ],
   "source": [
    "print(\" Running NLP analysis on all scraped articles...\")\n",
    "analysis_results = data[\"Article_Text\"].apply(analyze_text).apply(pd.Series)\n",
    "\n",
    "# Merge with main DataFrame\n",
    "data = pd.concat([data, analysis_results], axis=1)\n",
    "\n",
    "print(\" NLP analysis complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1cdf297d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Final file saved successfully as: Output_Data_Structure_Completed.xlsx\n"
     ]
    }
   ],
   "source": [
    "output_path = \"Output_Data_Structure_Completed.xlsx\"\n",
    "data.to_excel(output_path, index=False)\n",
    "print(f\" Final file saved successfully as: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "80069a2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URL_ID</th>\n",
       "      <th>URL</th>\n",
       "      <th>Article_Title</th>\n",
       "      <th>Article_Text</th>\n",
       "      <th>POSITIVE SCORE</th>\n",
       "      <th>NEGATIVE SCORE</th>\n",
       "      <th>POLARITY SCORE</th>\n",
       "      <th>SUBJECTIVITY SCORE</th>\n",
       "      <th>AVG SENTENCE LENGTH</th>\n",
       "      <th>PERCENTAGE OF COMPLEX WORDS</th>\n",
       "      <th>FOG INDEX</th>\n",
       "      <th>AVG NUMBER OF WORDS PER SENTENCE</th>\n",
       "      <th>COMPLEX WORD COUNT</th>\n",
       "      <th>WORD COUNT</th>\n",
       "      <th>SYLLABLE PER WORD</th>\n",
       "      <th>PERSONAL PRONOUNS</th>\n",
       "      <th>AVG WORD LENGTH</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bctech2011</td>\n",
       "      <td>https://insights.blackcoffer.com/ml-and-ai-bas...</td>\n",
       "      <td>ML and AI-based insurance premium model to pre...</td>\n",
       "      <td>Client Background\\n\\n\\nClient:\\n A leading ins...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.463277</td>\n",
       "      <td>0.507463</td>\n",
       "      <td>3.988296</td>\n",
       "      <td>9.463277</td>\n",
       "      <td>850.0</td>\n",
       "      <td>1675.0</td>\n",
       "      <td>2.660299</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.948060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bctech2012</td>\n",
       "      <td>https://insights.blackcoffer.com/streamlined-i...</td>\n",
       "      <td>Streamlined Integration: Interactive Brokers A...</td>\n",
       "      <td>Client Background\\n\\n\\nClient:\\n A leading fin...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.269231</td>\n",
       "      <td>0.438650</td>\n",
       "      <td>2.683152</td>\n",
       "      <td>6.269231</td>\n",
       "      <td>143.0</td>\n",
       "      <td>326.0</td>\n",
       "      <td>2.631902</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.846626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bctech2013</td>\n",
       "      <td>https://insights.blackcoffer.com/efficient-dat...</td>\n",
       "      <td>Efficient Data Integration and User-Friendly I...</td>\n",
       "      <td>Client Background\\n\\n\\nClient:\\n A leading tec...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.542857</td>\n",
       "      <td>0.349010</td>\n",
       "      <td>4.756747</td>\n",
       "      <td>11.542857</td>\n",
       "      <td>141.0</td>\n",
       "      <td>404.0</td>\n",
       "      <td>2.306931</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.264851</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       URL_ID                                                URL  \\\n",
       "0  bctech2011  https://insights.blackcoffer.com/ml-and-ai-bas...   \n",
       "1  bctech2012  https://insights.blackcoffer.com/streamlined-i...   \n",
       "2  bctech2013  https://insights.blackcoffer.com/efficient-dat...   \n",
       "\n",
       "                                       Article_Title  \\\n",
       "0  ML and AI-based insurance premium model to pre...   \n",
       "1  Streamlined Integration: Interactive Brokers A...   \n",
       "2  Efficient Data Integration and User-Friendly I...   \n",
       "\n",
       "                                        Article_Text  POSITIVE SCORE  \\\n",
       "0  Client Background\\n\\n\\nClient:\\n A leading ins...             0.0   \n",
       "1  Client Background\\n\\n\\nClient:\\n A leading fin...             0.0   \n",
       "2  Client Background\\n\\n\\nClient:\\n A leading tec...             0.0   \n",
       "\n",
       "   NEGATIVE SCORE  POLARITY SCORE  SUBJECTIVITY SCORE  AVG SENTENCE LENGTH  \\\n",
       "0             0.0             0.0                 0.0             9.463277   \n",
       "1             0.0             0.0                 0.0             6.269231   \n",
       "2             0.0             0.0                 0.0            11.542857   \n",
       "\n",
       "   PERCENTAGE OF COMPLEX WORDS  FOG INDEX  AVG NUMBER OF WORDS PER SENTENCE  \\\n",
       "0                     0.507463   3.988296                          9.463277   \n",
       "1                     0.438650   2.683152                          6.269231   \n",
       "2                     0.349010   4.756747                         11.542857   \n",
       "\n",
       "   COMPLEX WORD COUNT  WORD COUNT  SYLLABLE PER WORD  PERSONAL PRONOUNS  \\\n",
       "0               850.0      1675.0           2.660299                2.0   \n",
       "1               143.0       326.0           2.631902                1.0   \n",
       "2               141.0       404.0           2.306931                1.0   \n",
       "\n",
       "   AVG WORD LENGTH  \n",
       "0         7.948060  \n",
       "1         7.846626  \n",
       "2         7.264851  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "948eb47d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sunik\\Output_Data_Structure_Completed.csv\n"
     ]
    }
   ],
   "source": [
    "OUTPUT_SAVE = DATA_DIR / \"Output_Data_Structure_Completed.xlsx\"\n",
    "OUTPUT_CSV = DATA_DIR / \"Output_Data_Structure_Completed.csv\"\n",
    "\n",
    "print(OUTPUT_CSV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "30b478a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23a2ca46",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
